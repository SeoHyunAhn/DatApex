{"version":3,"file":"static/webpack/static/development/pages/mlOptions.js.fb0d0b0db57fc3b1398e.hot-update.js","sources":["webpack:///./pages/mlOptions.js"],"sourcesContent":["import React, { Component } from \"react\";\nimport Layout from \"../components/layout\";\n// import Link from 'next/link';\nimport { Link, Router } from \"../routes\";\n\nconst styleCard = {\n  marginBottom: \"10px\"\n};\n\nclass MlOptions extends Component {\n  render() {\n    return (\n      <>\n        <h1>MLOptions</h1>\n        <hr></hr>\n        <div className=\"container-fluid\">\n          <div className=\"row\">\n            <div className=\"col-6\">\n              <div className=\"card\" style={styleCard}>\n                <div className=\"card-header\">Naive Bayes algorithm</div>\n                <div className=\"card-body\">\n                  <img\n                    className=\"card-img\"\n                    variant=\"top\"\n                    src=\"../static/nba.png\"\n                  />\n                  <h5 className=\"card-title\">Explanation: </h5>\n                  <p className=\"card-text\">\n                    Naïve Bayes is a classification technique/algorithm based on\n                    Bayes’ Theorem with an assumption of independence among the\n                    predictors. Loosely translated, this means that a Naïve\n                    Bayes classifier assumes that the presence of a particular\n                    feature in a class is unrelated to the presence of any other\n                    feature. This model is particularly useful for large data\n                    sets – along with simplicity, Naïve Bayes is known to\n                    outperform even highly sophisticated classification methods\n                    in some cases. <br></br>The Naïve Bayes algorithm works by\n                    converting the dataset into a frequency table, creating a\n                    likelihood table by calculating probabilities & then using\n                    the Naïve Bayes equation to calculate the posterior\n                    probability for each class. The class with the highest\n                    posterior probability is considered the outcome of the\n                    prediction. This algorithm is typically used in text\n                    classification & with problems having multiple classes.\n                  </p>\n\n                  <Link route=\"mlalgo\" params={{ path: \"nba\" }}>\n                    <a className=\"btn btn-primary\">Go somewhere</a>\n                  </Link>\n\n                  {/* <Link href={{ pathname: 'mlalgo', query: { path: \"nba\" }}}>\n                    <a className=\"btn btn-primary\">Go somewhere</a>\n                  </Link> */}\n                </div>\n              </div>\n            </div>\n            <div className=\"col-6\">\n              <div className=\"card\" style={styleCard}>\n                <div className=\"card-header\">Logistic Regression algorithm</div>\n                <div className=\"card-body\">\n                  <img\n                    className=\"card-img\"\n                    variant=\"top\"\n                    src=\"../static/lra.png\"\n                  />\n                  <h5 className=\"card-title\">Explanation: </h5>\n                  <p className=\"card-text\">\n                    Logistic Regression is a classification machine-learning\n                    algorithm used to assign observations to a discrete set of\n                    classes (binary or multiple classes). Logistic regression\n                    transforms its output using the logistic sigmoid function to\n                    return a probability value. Logistic Regression is a\n                    predictive analysis algorithm as it is based on the concepts\n                    of probability. It uses a more complex cost function than\n                    linear regression (the logistic or sigmoid function).\n                    <br></br>\n                    In order to map predicted values to probabilities, we use\n                    the Sigmoid function which maps any real value into another\n                    value between 0 & 1.\n                  </p>\n                  <Link route=\"mlalgo\" params={{ path: \"lra\" }}>\n                    <a className=\"btn btn-primary\">Go somewhere</a>\n                  </Link>\n                </div>\n              </div>\n            </div>\n            <div className=\"col-6\">\n              <div className=\"card\" style={styleCard}>\n                <div className=\"card-header\">\n                  Support Vector Machine algorithm\n                </div>\n                <div className=\"card-body\">\n                  <img\n                    className=\"card-img\"\n                    variant=\"top\"\n                    src=\"../static/svm.png\"\n                  />\n                  <h5 className=\"card-title\">Explanation: </h5>\n                  <p className=\"card-text\">\n                    SVM is a supervised machine learning algorithm which may be\n                    used for either classification or regression problems.\n                    However, in general, it is used predominantly for\n                    classification problems. In this algorithm, we plot each\n                    data item as a point in n-dimensional space (n being the\n                    number of features) with the value of each feature being the\n                    value of a particular coordinate. Subsequently,\n                    classification is performed by finding the hyperplane (the\n                    equivalent of a line in a 2-dimensional space) that\n                    differentiates the two classes well. <br></br>Above image\n                    shows, Support Vectors are simply the co-ordinates of\n                    individual observations. SVM is a frontier which best\n                    segregates the two classes.\n                  </p>\n                  <Link route=\"mlalgo\" params={{ path: \"svm\" }}>\n                    <a className=\"btn btn-primary\">Go somewhere</a>\n                  </Link>\n                </div>\n              </div>\n            </div>\n            <div className=\"col-6\">\n              <div className=\"card\" style={styleCard}>\n                <div className=\"card-header\">Bagging algorithm</div>\n                <div className=\"card-body\">\n                  <img\n                    className=\"card-img\"\n                    variant=\"top\"\n                    src=\"../static/bag.png\"\n                  />\n                  <h5 className=\"card-title\">Explanation: </h5>\n                  <p className=\"card-text\">\n                    Bagging algorithm is a simple yet powerful ensemble method.\n                    An ensemble method is one that combines predictions from\n                    multiple machine learning algorithms together to make more\n                    accurate predictions than any individual method.\n                    <br />\n                    This method is especially helpful in reducing the variance\n                    for algorithms which tend to have really high variance.\n                    Bagging involves applying the Bootstrapping procedure to a\n                    high-variance machine learning algorithm (typically decision\n                    trees).\n                    <br />\n                    <br />\n                    Let’s assume we have a sample dataset of 1000 instances & we\n                    are using the CART algorithm. Bagging would work as follows:\n                    <br />- Create several (e.g. 100) random sub-samples of the\n                    dataset with replacement.\n                    <br />- Train a CART model on each sample.\n                    <br />- Given a new dataset, calculate the average\n                    prediction from each model.\n                    <br />\n                    <br />\n                    Bagging may be used for either classification or regression\n                    problems.\n                  </p>\n                  <Link route=\"mlalgo\" params={{ path: \"bag\" }}>\n                    <a className=\"btn btn-primary\">Go somewhere</a>\n                  </Link>\n                </div>\n              </div>\n            </div>\n            <div className=\"col-6\">\n              <div className=\"card\" style={styleCard}>\n                <div className=\"card-header\">K-means Clustering algorithm</div>\n                <div className=\"card-body\">\n                  <img\n                    className=\"card-img\"\n                    variant=\"top\"\n                    src=\"../static/clus.png\"\n                  />\n                  <h5 className=\"card-title\">Explanation: </h5>\n                  <p className=\"card-text\">\n                    K-means Clustering is one of the simplest and most popular\n                    unsupervised machine learning algorithms. Typically,\n                    unsupervised machine learning algorithms make inferences\n                    from datasets using only input vectors without referring to\n                    known/labelled outcomes.\n                    <br />\n                    A cluster, simply put, refers to a collection of data points\n                    aggregated together because of certain similarities. The ‘k’\n                    from the name refers to the number of centroids needed in\n                    the dataset, and is defined by us. A centroid is the\n                    imaginary or real location representing the center of the\n                    cluster. The ‘means’ in the K-Means refers to the averaging\n                    of data (i.e. finding the centroid).\n                    <br />\n                    <br />\n                    The algorithm starts with a first group of randomly selected\n                    centroids that are used as beginning points for every\n                    cluster, and then performs iterative calculations to\n                    optimize the positions of the centroids. It stops creating\n                    or optimizing clusters when:\n                    <br />- The centroids have stabilized, i.e. there is no\n                    change in their values because the clustering has been\n                    successful;\n                    <br />- The defined number of iterations has been reached.\n                  </p>\n                  <Link route=\"mlalgo\" params={{ path: \"clus\" }}>\n                    <a className=\"btn btn-primary\">Go somewhere</a>\n                  </Link>\n                </div>\n              </div>\n            </div>\n            <div className=\"col-6\">\n              <div className=\"card\" style={styleCard}>\n                <div className=\"card-header\">Decision Tree algorithm</div>\n                <div className=\"card-body\">\n                  <img\n                    className=\"card-img\"\n                    variant=\"top\"\n                    src=\"../static/tree.png\"\n                  />\n                  <h5 className=\"card-title\">Explanation: </h5>\n                  <p className=\"card-text\">\n                    Decision Tree algorithm belongs to the family of supervised\n                    machine learning algorithms. Unlike other supervised\n                    learning algorithms, this algorithm can be used for solving\n                    regression & classification problems too. The general model\n                    of Decision Trees algorithm is to create a training model\n                    which can be used to predict the class or value of target\n                    variables by learning decision rules inferred from the\n                    training data.\n                    <br />\n                    <br />\n                    As the name insinuates, this algorithm uses a tree\n                    representation to solve the problem, wherein each internal\n                    node corresponds to an attribute while each leaf node\n                    corresponds to a class label. A rough outline of the\n                    algorithm is as follows:\n                    <br />- Place the best attribute of the dataset at the root\n                    of the tree.\n                    <br />- Split the training set into subsets in such a way\n                    that each subset contains data with the same value for an\n                    attribute.\n                    <br />- Repeat the previous two steps on each subset until\n                    leaf nodes are found in all branches of the tree.\n                    <br />\n                    <br />\n                    With decision trees, one of the biggest challenges is to\n                    identify which attributes need to be considered as the root\n                    node and each level.\n                  </p>\n                  <Link route=\"mlalgo\" params={{ path: \"tree\" }}>\n                    <a className=\"btn btn-primary\">Go somewhere</a>\n                  </Link>\n                </div>\n              </div>\n            </div>\n          </div>\n        </div>\n      </>\n    );\n  }\n}\n\nexport default MlOptions;\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAA;AACA;AACA;AACA;AAEA;AACA;AADA;AACA;AAGA;;;;;;;;;;;;;AACA;AACA;AAEA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA;AACA;AACA;AAHA;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AASA;AAAA;AAAA;AAAA;AAAA;AAAA;AAUA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AASA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA;AACA;AACA;AAHA;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AASA;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA;AACA;AACA;AAHA;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAUA;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA;AACA;AACA;AAHA;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA;AAAA;AAAA;AAAA;AAAA;AAAA;AAMA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAIA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA;AACA;AACA;AAHA;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAMA;AAAA;AAAA;AAAA;AAAA;AAAA;AAQA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAMA;AAAA;AAAA;AAAA;AAAA;AAAA;AAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA;AACA;AACA;AAHA;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AASA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAMA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA;AAAA;AAAA;AAAA;AAAA;AAAA;AAGA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AASA;;;;AAlPA;AACA;AAoPA;;;;A","sourceRoot":""}